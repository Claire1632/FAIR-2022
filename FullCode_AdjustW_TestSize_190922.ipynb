{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.20356986, -0.23616822, -0.01535438, ..., -0.10297836,\n",
      "        -0.00333545,  1.9115189 ],\n",
      "       [-0.71319985, -0.07417361,  0.33070229, ..., -0.10297836,\n",
      "        -1.03616576, -0.16111872],\n",
      "       [-0.42198271,  0.16881831, -1.05352439, ..., -0.10297836,\n",
      "         0.16880293, -0.53796193],\n",
      "       ...,\n",
      "       [ 0.59727728,  0.08782101,  0.67675896, ..., -0.10297836,\n",
      "        -2.75754961, -0.53796193],\n",
      "       [-0.13076557, -0.39816283,  1.48422452, ..., -0.10297836,\n",
      "         1.37377162, -0.53796193],\n",
      "       [ 0.014843  ,  0.08782101,  0.79211118, ..., -0.10297836,\n",
      "        -0.34761222, -0.53796193]]), array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,\n",
      "       -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "        1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1],\n",
      "      dtype=int64), array([[ 0.67008156,  1.2217833 ,  0.21535006, ..., -0.10297836,\n",
      "        -0.69188899,  0.30993528],\n",
      "       [-2.24208984, -0.72215206,  0.56140673, ..., -0.10297836,\n",
      "         0.34094132, -0.25532953],\n",
      "       [-1.58685127, -0.31716553, -2.78380773, ..., -0.10297836,\n",
      "        -1.89685769, -0.53796193],\n",
      "       ...,\n",
      "       [ 0.37886442,  0.24981562,  0.67675896, ..., -0.10297836,\n",
      "        -0.34761222, -0.53796193],\n",
      "       [-0.494787  , -0.64115475,  0.44605451, ..., -0.10297836,\n",
      "         1.20163324,  0.49835688],\n",
      "       [-0.494787  ,  0.08782101,  0.44605451, ..., -0.10297836,\n",
      "        -0.86402738, -0.16111872]]), array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n",
      "      dtype=int64), array([[-0.64039557, -0.39816283,  0.33070229, ..., -0.10297836,\n",
      "         0.16880293, -0.25532953],\n",
      "       [-0.13076557,  0.65480215, -0.36141105, ..., -0.10297836,\n",
      "         0.5130797 ,  4.36099973],\n",
      "       [-1.9508727 , -2.4230955 ,  0.33070229, ..., -0.10297836,\n",
      "        -1.03616576, -0.53796193],\n",
      "       ...,\n",
      "       [-0.05796129,  0.16881831,  0.33070229, ..., -0.10297836,\n",
      "         0.16880293, -0.53796193],\n",
      "       [ 0.014843  , -0.31716553,  0.21535006, ..., -0.10297836,\n",
      "         0.34094132, -0.16111872],\n",
      "       [ 0.67008156,  1.38377791,  0.67675896, ..., -0.10297836,\n",
      "        -0.51975061, -0.53796193]]), array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,\n",
      "       -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1,  1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
      "       -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data import Vertebral_column\n",
    "from data import Co_Author\n",
    "from Processing_Data import Abanole\n",
    "from Processing_Data import Ecoli, Ecoli_All\n",
    "from Processing_Data import Ecloli1\n",
    "from Processing_Data import Ecoli3\n",
    "from Processing_Data import Glass1\n",
    "from Processing_Data import Glass4\n",
    "from Processing_Data import Haberman\n",
    "from Processing_Data import Waveform\n",
    "from Processing_Data import New_thyroid2\n",
    "from Processing_Data import Page_blocks\n",
    "from Processing_Data import Pima_Indians_Diabetes, Pima\n",
    "from Processing_Data import Satimage\n",
    "from Processing_Data import Transfusion\n",
    "from Processing_Data import Yeast\n",
    "from Processing_Data import Haberman_All\n",
    "from Processing_Data import Transfusion_All\n",
    "from Processing_Data import PimaIndians_All\n",
    "from data import indian_liver_patient\n",
    "#from data import spect_heart\n",
    "from wsvm.application import Wsvm\n",
    "from svm.application import Svm\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.metrics import f1_score\n",
    "from sklearn.metrics  import classification_report,precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,roc_auc_score,f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from datetime import datetime\n",
    "from fuzzy.weight import fuzzy\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from Processing_Data import Ecoli_Kfold\n",
    "from Processing_Data import Haberman_KFold\n",
    "from Processing_Data import Transfution_Kfold\n",
    "from Processing_Data import Co_Author\n",
    "from Processing_Data import Co_Author_TestSize\n",
    "import csv\n",
    "from data import Vertebral_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_lib(X_train, y_train,X_test):\n",
    "    svc=SVC(probability=True, kernel='linear')\n",
    "    model = svc.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wsvm(C,X_train, y_train,X_test,distribution_weight=None):\n",
    "    model = Wsvm(C,distribution_weight)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(C,X_train, y_train,X_test):\n",
    "    model = Svm(C)\n",
    "    model.fit(X_train, y_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tomek(X,y, class_type):\n",
    "    print(y)\n",
    "    print(type(y))\n",
    "    nn = NearestNeighbors(n_neighbors=2)\n",
    "    nn.fit(X)\n",
    "    nn_index = nn.kneighbors(X, return_distance=False)[:, 1]\n",
    "    links = np.zeros(len(y), dtype=bool)\n",
    "    # find which class to not consider\n",
    "    class_excluded = [c for c in np.unique(y) if c not in class_type]\n",
    "    X_dangxet = []\n",
    "    X_tl = []\n",
    "    # there is a Tomek link between two samples if they are both nearest\n",
    "    # neighbors of each others.\n",
    "    for index_sample, target_sample in enumerate(y):\n",
    "        if target_sample in class_excluded:\n",
    "            continue\n",
    "        if y[nn_index[index_sample]] != target_sample:\n",
    "            if nn_index[nn_index[index_sample]] == index_sample:\n",
    "                X_tl.append(index_sample)\n",
    "                X_dangxet.append(nn_index[index_sample])\n",
    "                links[index_sample] = True\n",
    "                \n",
    "    return links,X_dangxet,X_tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_tomek_new(X,y, class_type):\n",
    "    # print(y)\n",
    "    # print(type(y))\n",
    "    nn = NearestNeighbors(n_neighbors=2)\n",
    "    nn.fit(X)\n",
    "    nn_index = nn.kneighbors(X, return_distance=False)[:, 1]\n",
    "    links = np.zeros(len(y), dtype=bool)\n",
    "    # find which class to not consider\n",
    "    class_excluded = [c for c in np.unique(y) if c not in class_type]\n",
    "    X_dangxet = []\n",
    "    X_tl = []\n",
    "    # there is a Tomek link between two samples if they are both nearest\n",
    "    # neighbors of each others.\n",
    "    for index_sample, target_sample in enumerate(y):\n",
    "        if target_sample in class_excluded:\n",
    "            continue\n",
    "        if y[nn_index[index_sample]] != target_sample:\n",
    "            if nn_index[nn_index[index_sample]] == index_sample:\n",
    "                X_tl.append(index_sample)\n",
    "                X_dangxet.append(nn_index[index_sample])\n",
    "                links[index_sample] = True\n",
    "\n",
    "    stt = np.zeros(len(X_dangxet), dtype=int)\n",
    "    arr_tlp = np.stack((X_dangxet, X_tl, stt), axis=1)\n",
    "\n",
    "    return arr_tlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "<class 'numpy.ndarray'>\n",
      "[[183 118   0]\n",
      " [182 223   0]\n",
      " [210 227   0]\n",
      " [178 230   0]\n",
      " [200 239   0]\n",
      " [158 245   0]\n",
      " [157 252   0]\n",
      " [180 254   0]\n",
      " [205 314   0]]\n"
     ]
    }
   ],
   "source": [
    "X, y = Ecoli_Kfold.load_data()\n",
    "arr = is_tomek_new(X, y, class_type=[-1.0])\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "<class 'numpy.ndarray'>\n",
      "[183, 182, 210, 178, 200, 158, 157, 180, 205]\n",
      "[118, 223, 227, 230, 239, 245, 252, 254, 314]\n",
      "[[0.16 0.51 0.48 0.5  0.33 0.39 0.48]\n",
      " [0.6  0.5  1.   0.5  0.54 0.77 0.8 ]\n",
      " [0.69 0.39 0.48 0.5  0.57 0.76 0.79]\n",
      " [0.83 0.37 0.48 0.5  0.61 0.71 0.74]\n",
      " [0.58 0.55 0.48 0.5  0.57 0.7  0.74]\n",
      " [0.48 0.45 0.48 0.5  0.6  0.78 0.8 ]\n",
      " [0.84 0.44 0.48 0.5  0.48 0.71 0.74]\n",
      " [0.63 0.54 0.48 0.5  0.65 0.79 0.81]\n",
      " [0.61 0.66 0.48 0.5  0.46 0.87 0.88]]\n",
      "[0.16 0.51 0.48 0.5  0.33 0.39 0.48]\n",
      "[0.6  0.5  1.   0.5  0.54 0.77 0.8 ]\n",
      "[0.69 0.39 0.48 0.5  0.57 0.76 0.79]\n",
      "[0.83 0.37 0.48 0.5  0.61 0.71 0.74]\n",
      "[0.58 0.55 0.48 0.5  0.57 0.7  0.74]\n",
      "[0.48 0.45 0.48 0.5  0.6  0.78 0.8 ]\n",
      "[0.84 0.44 0.48 0.5  0.48 0.71 0.74]\n",
      "[0.63 0.54 0.48 0.5  0.65 0.79 0.81]\n",
      "[0.61 0.66 0.48 0.5  0.46 0.87 0.88]\n"
     ]
    }
   ],
   "source": [
    "X, y = Ecoli_Kfold.load_data()\n",
    "X = np.array(X)\n",
    "links, positive_ind, negative_ind = is_tomek(X, y, class_type=[-1.0])\n",
    "# links.tolist()\n",
    "print(positive_ind)\n",
    "print(negative_ind)\n",
    "pos_index = np.where(y == 1)[0]\n",
    "neg_index = np.where(y == -1)[0]\n",
    "print(X[positive_ind])\n",
    "for ind,i in enumerate(positive_ind): \n",
    "    # print(ind, i)\n",
    "    print(X[i])\n",
    "# stt = np.zeros(len(positive_ind), dtype=int)\n",
    "# con = np.stack((positive_ind, negative_ind,stt), axis=1)\n",
    "# print(con)\n",
    "# print(con[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gmean(y_test,y_pred):\n",
    "    cm_WSVM = metrics.confusion_matrix(y_test, y_pred)\n",
    "    sensitivity = cm_WSVM[1,1]/(cm_WSVM[1,0]+cm_WSVM[1,1])\n",
    "    specificity = cm_WSVM[0,0]/(cm_WSVM[0,0]+cm_WSVM[0,1])\n",
    "    gmean = math.sqrt(sensitivity*specificity)\n",
    "    return specificity,sensitivity,gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metr(X_train,y_test,test_pred,se,sp,gmean):\n",
    "    #se,sp,gmean = Gmean(y_test,test_pred)\n",
    "    print(\"So luong samples: \",len(X_train))\n",
    "    print(\"\\n\",classification_report(y_test, test_pred))\n",
    "    print(\"SP      : \",sp)\n",
    "    print(\"SE      : \",se)\n",
    "    print(\"Gmean   : \",gmean)\n",
    "    print(\"F1 Score: \",f1_score(y_test, test_pred))\n",
    "    print(\"Accuracy: \",accuracy_score(y_test,test_pred))\n",
    "    print(\"AUC     : \",roc_auc_score(y_test, test_pred))\n",
    "    print(\"Ma tran nham lan: \\n\",confusion_matrix(y_test, test_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metr_text(f,X_train,y_test,test_pred,sp,se,gmean):\n",
    "    #se,sp,gmean = Gmean(y_test,test_pred)\n",
    "    f.write(f\"\\n\\nSo luong samples Tong: {len(X_train)+len(y_test)}\")\n",
    "    f.write(f\"\\n\\nSo luong samples training: {len(X_train)}\")\n",
    "    f.write(f\"\\nSo luong samples testing: {len(y_test)}\\n\")\n",
    "    f.write(\"\\n\"+str(classification_report(y_test, test_pred)))\n",
    "    f.write(f\"\\nSP      : {sp:0.4f}\")\n",
    "    f.write(f\"\\nSE      : {se:0.4f}\")\n",
    "    f.write(f\"\\nGmean   : {gmean:0.4f}\")\n",
    "    f.write(f\"\\nF1 Score: {f1_score(y_test, test_pred):0.4f}\")\n",
    "    f.write(f\"\\nAccuracy: {accuracy_score(y_test,test_pred):0.4f}\")\n",
    "    f.write(f\"\\nAUC     : {roc_auc_score(y_test, test_pred):0.4f}\")\n",
    "    f.write(\"\\n\\nMa tran nham lan: \\n\"+str(confusion_matrix(y_test, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight(X, y,name_method =\"actual_hyper_lin\", name_function = \"exp\", beta = None,C = None, gamma = None, u = None, sigma = None):\n",
    "    method = fuzzy.method()\n",
    "    function = fuzzy.function()\n",
    "    pos_index = np.where(y == 1)[0]\n",
    "    neg_index = np.where(y == -1)[0]\n",
    "    try:\n",
    "        if name_method == \"own_class_center\": \n",
    "            d = method.own_class_center(X, y)\n",
    "        elif name_method == \"estimated_hyper_lin\": # actual_hyper_lin, own_class_center\n",
    "            d = method.estimated_hyper_lin(X, y)\n",
    "        elif name_method == \"own_class_center_opposite\":\n",
    "            d = method.own_class_center_opposite(X, y)\n",
    "        elif name_method == 'actual_hyper_lin':\n",
    "            d = method.actual_hyper_lin(X, y,C = C, gamma = gamma)\n",
    "        elif name_method == 'own_class_center_divided':\n",
    "            d = method.own_class_center_divided(X, y)\n",
    "        elif name_method == \"distance_center_own_opposite_tam\":\n",
    "            d_own, d_opp, d_tam = method.distance_center_own_opposite_tam(X,y)\n",
    "        else:\n",
    "            print('dont exist method')\n",
    "        \n",
    "        if name_function == \"lin\":\n",
    "            W = function.lin(d)\n",
    "        elif name_function == \"exp\":\n",
    "            W = function.exp(d, beta)\n",
    "        elif name_function == \"lin_center_own\":\n",
    "            W = function.lin_center_own(d, pos_index,neg_index)\n",
    "        elif name_function == 'gau':\n",
    "            W = function.gau(d, u, sigma)\n",
    "        elif name_function == \"func_own_opp_new\":\n",
    "            W = function.func_own_opp_new(d_own,d_opp,pos_index,neg_index,d_tam)\n",
    "        elif name_function == \"func_own_opp_new_v1\":\n",
    "            W = function.func_own_opp_new_v1(d_own,d_opp,pos_index,neg_index,d_tam)\n",
    "        elif name_function == \"func_own_opp_new_v2\":\n",
    "            W = function.func_own_opp_new_v2(d_own,d_opp,pos_index,neg_index,d_tam)\n",
    "    except Exception as e:\n",
    "        print('dont exist function')\n",
    "        print(e)\n",
    "    # pos_index = np.where(y == 1)[0]\n",
    "    # neg_index = np.where(y == -1)[0]\n",
    "    r_pos = 1\n",
    "    r_neg = len(pos_index)/len(neg_index)\n",
    "    m = []\n",
    "    W = np.array(W)\n",
    "    m = W[pos_index]*r_pos\n",
    "    m = np.append(m, W[neg_index]*r_neg)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1088733  1.13578167 0.50990195 0.62417946 0.97447422 0.8219489\n",
      " 0.67082039 0.88746831]\n",
      "1.1357816691600546\n",
      "0.6241794613730894\n",
      "[3.35410197 2.36845941 3.43214219 2.95803989 2.6925824  3.09515751\n",
      " 3.68640747 3.93700394]\n",
      "3.9370039370059056\n",
      "2.368459414893994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.67699534, 0.81852059, 0.80138252]),\n",
       " array([0.48925437, 0.48390272, 0.56538842, 0.49739736, 0.39493747]),\n",
       " array([0.48925437, 0.67699534, 0.81852059, 0.48390272, 0.56538842,\n",
       "        0.49739736, 0.80138252, 0.39493747]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from Processing_Data import Test\n",
    "X, y = Test.load_data()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "pos_ind = np.where(y == 1)[0]\n",
    "neg_ind = np.where(y == -1)[0]\n",
    "def distance_center_own_opposite_tam(X,y):\n",
    "    X_own = copy.deepcopy(X)\n",
    "    X_opposite = copy.deepcopy(X)\n",
    "    pos_index = np.where(y == 1)[0]\n",
    "    neg_index = np.where(y == -1)[0]\n",
    "    x_cenpos = np.mean(X[pos_index], axis=0)\n",
    "    x_cenneg = np.mean(X[neg_index], axis=0)\n",
    "\n",
    "    # compute distance\n",
    "    X_own[pos_index] = X_own[pos_index] - x_cenpos\n",
    "    X_own[neg_index] = X_own[neg_index] - x_cenneg\n",
    "\n",
    "    X_opposite[pos_index] = X_opposite[pos_index] - x_cenneg\n",
    "    X_opposite[neg_index] = X_opposite[neg_index] - x_cenpos\n",
    "\n",
    "    x_2tam = x_cenneg - x_cenpos\n",
    "\n",
    "    d_cen_own = np.linalg.norm(X_own, axis=1)\n",
    "    d_cen_opposite = np.linalg.norm(X_opposite, axis=1)\n",
    "    d_tam = np.linalg.norm(x_2tam, axis=0)\n",
    "    return d_cen_own, d_cen_opposite, d_tam\n",
    "distance_center_own_opposite_tam(X,y)\n",
    "\n",
    "d_cenpos, d_cenneg, d_tam = distance_center_own_opposite_tam(X,y)\n",
    "dmax_pos = np.max(d_cenpos[pos_ind])\n",
    "dmin_pos = np.min(d_cenpos[neg_ind])\n",
    "dmax_neg = np.max(d_cenneg[neg_ind])\n",
    "dmin_neg = np.min(d_cenneg[pos_ind])\n",
    "\n",
    "print(d_cenpos)\n",
    "print(dmax_pos)\n",
    "print(dmin_pos)\n",
    "\n",
    "print(d_cenneg)\n",
    "print(dmax_neg)\n",
    "print(dmin_neg)\n",
    "\n",
    "def func_own_opp_new_v1(d_cenpos, d_cenneg, pos_ind, neg_ind, d_tam, delta=1e-6):\n",
    "    f = np.zeros(len(d_cenpos))\n",
    "    dmax_pos = np.max(d_cenpos[pos_ind])\n",
    "    dmin_pos = np.min(d_cenpos[neg_ind])\n",
    "    dmax_neg = np.max(d_cenneg[neg_ind])\n",
    "    dmin_neg = np.min(d_cenneg[pos_ind])\n",
    "    f[pos_ind] = 1 - (d_cenpos[pos_ind] + dmin_neg/d_cenneg[pos_ind])/(dmax_pos+dmin_neg+d_tam+delta)\n",
    "    f[neg_ind] = 1 - (d_cenneg[neg_ind] + dmin_pos/d_cenpos[neg_ind])/(dmax_neg+dmin_pos+d_tam+delta)\n",
    "    return f[pos_ind], f[neg_ind], f\n",
    "func_own_opp_new_v1(d_cenpos, d_cenneg, pos_ind, neg_ind, d_tam, delta=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction):\n",
    "    if namemethod ==\"own_class_center_opposite\" and namefunction == \"exp\":\n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction,beta = beta_center)\n",
    "    elif namemethod ==\"own_class_center\" and namefunction == \"exp\":\n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction,beta = beta_estimate)\n",
    "    elif namemethod ==\"own_class_center_divided\" and namefunction == \"exp\":\n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction,beta = beta_estimate)\n",
    "    elif namemethod ==\"estimated_hyper_lin\" and namefunction == \"exp\":\n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction,beta = beta_estimate)\n",
    "    elif namemethod ==\"actual_hyper_lin\" and namefunction == \"exp\":\n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction,beta = beta_actual)\n",
    "    else:   \n",
    "        distribution_weight = compute_weight(X_train, y_train,name_method = namemethod,name_function = namefunction)\n",
    "    return distribution_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5], dtype=int64),)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([5,6,7,8,9,10])\n",
    "a = np.where(y>5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tomelinks(f,C,weight,X_test,y_test,X_train,y_train,n_neighbors,clf=None,namemethod=None,namefunction=None):\n",
    "    links,ind_posX,ind_negX = is_tomek(X_train,y_train,class_type=[-1.0])\n",
    "    #print(len(ind_posX))\n",
    "    new_W = weight\n",
    "    pos_index = np.where(y_train == 1)[0]\n",
    "    neg_index = np.where(y_train == -1)[0]\n",
    "    clf = Wsvm(C,new_W)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    specificity,sensitivity,gmean = Gmean(y_test,y_predict)\n",
    "    nn2 = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn2.fit(X_train)\n",
    "    y_nn = [] #chứa nhãn của k mẫu dữ liệu gần nhất với mẫu được xét\n",
    "\n",
    "    # Mẫu âm bị phân loại sai -> Giảm trọng số của mẫu âm đó\n",
    "    neg_pred = clf.predict(X_train[neg_index])\n",
    "    idx_neg_wrong = np.where(neg_pred != -1.0)\n",
    "    new_W[idx_neg_wrong] =  new_W[idx_neg_wrong]*0.5 # giam manh (Co-Author*0.9)\n",
    "\n",
    "    # Tăng, giảm trọng số của các mẫu trong TLPs\n",
    "    ind_nn = [] # chứa chỉ số của các mẫu dương bị phân loại sai trong ind_posX\n",
    "    for ind,i in enumerate(ind_posX): \n",
    "        y_pred = clf.predict([X_train[i]])  #\n",
    "        if y_pred == -1.0: # dương bị phân loại sai\n",
    "            ind_nn.append(ind)                          \n",
    "            knn_X = (nn2.kneighbors([X_train[i]])[1]).tolist()  \n",
    "            for j in knn_X[0]:\n",
    "                y_nn.append(y_train[j])    # gom nhãn láng giềng của X_train[i] bị dự đoán sai vào y_nn\n",
    "        else: # dương được phân loại đúng\n",
    "            new_W[ind_negX[ind]] = new_W[ind_negX[ind]]/1.2 #Co-Author/1.05\n",
    "            new_W[i] = new_W[i]*1.2 #Co-Author*1.05\n",
    "    ind_nn = np.array(ind_nn)\n",
    "    #print(ind_nn)\n",
    "    y_nn = np.array(y_nn)\n",
    "    if len(y_nn)>0:\n",
    "        y_nn = np.array_split(y_nn, len(y_nn)/n_neighbors) \n",
    "    #print(len(y_nn))\n",
    "    for ind,i in enumerate(range(0,len(y_nn))):   #\n",
    "        if 1 not in y_nn[i][1:]:      # Nếu không có nhãn 1 xung quanh X_train[i] bị dự đoán sai => nhiễu dương -> giảm trọng số\n",
    "            new_W[ind_posX[ind_nn[ind]]] = new_W[ind_posX[ind_nn[ind]]]*0.5\n",
    "            #print(ind_posX[ind_nn[ind]])\n",
    "        else:\n",
    "            # print(\"Old Neg: \",new_W[ind_negX[ind_nn[ind]]])\n",
    "            # print(\"Old Pos: \",new_W[ind_posX[ind_nn[ind]]])\n",
    "            new_W[ind_negX[ind_nn[ind]]] = new_W[ind_negX[ind_nn[ind]]]/1.2\n",
    "            new_W[ind_posX[ind_nn[ind]]] = new_W[ind_posX[ind_nn[ind]]]*1.2\n",
    "            # print(\"New Neg: \",new_W[ind_negX[ind_nn[ind]]])\n",
    "            # print(\"New Pos: \",new_W[ind_posX[ind_nn[ind]]])\n",
    "\n",
    "    return new_W,gmean,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "<class 'numpy.ndarray'>\n",
      "[183, 182, 210, 178, 200, 158, 157, 180, 205]\n",
      "[118, 223, 227, 230, 239, 245, 252, 254, 314]\n",
      "25\n",
      "[1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0]\n",
      "[array([ 1., -1., -1., -1., -1.]), array([ 1., -1., -1., -1., -1.]), array([ 1., -1., -1., -1.,  1.]), array([ 1., -1., -1., -1., -1.]), array([ 1., -1., -1., -1., -1.])]\n",
      "range(0, 5)\n",
      "[-1. -1. -1. -1.]\n",
      "183\n",
      "[-1. -1. -1. -1.]\n",
      "182\n",
      "[-1. -1. -1. -1.]\n",
      "178\n",
      "[-1. -1. -1. -1.]\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = Ecoli_Kfold.load_data()\n",
    "X = np.array(X)\n",
    "y_nn = []\n",
    "ind_nn = []\n",
    "links, positive_ind, negative_ind = is_tomek(X, y, class_type=[-1.0])\n",
    "# links.tolist()\n",
    "print(positive_ind)\n",
    "print(negative_ind)\n",
    "pos_index = np.where(y == 1)[0]\n",
    "neg_index = np.where(y == -1)[0]\n",
    "nn2 = NearestNeighbors(n_neighbors=5)\n",
    "nn2.fit(X)\n",
    "clf = KNeighborsClassifier(n_neighbors= 9)\n",
    "clf.fit(X,y)\n",
    "for ind,i in enumerate(positive_ind): \n",
    "    y_pred = clf.predict([X[i]]) \n",
    "    if y_pred == -1.0: # dương bị phân loại sai\n",
    "        ind_nn.append(ind)\n",
    "    # print(negative_ind[ind])\n",
    "        knn_X = (nn2.kneighbors([X[i]])[1]).tolist()  \n",
    "        # print(knn_X)\n",
    "        for j in knn_X[0]:\n",
    "            y_nn.append(y[j])\n",
    "print(len(y_nn))\n",
    "print(y_nn)\n",
    "if len(y_nn)>0:\n",
    "    y_nn = np.array_split(y_nn, len(y_nn)/5) \n",
    "print(y_nn)\n",
    "\n",
    "print(range(0,len(y_nn)))\n",
    "for ind,i in enumerate(range(0,len(y_nn))):\n",
    "    if 1 not in y_nn[i][1:]:\n",
    "        print(y_nn[i][1:])\n",
    "        print(positive_ind[ind_nn[ind]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tomelinks_new(f,C,weight,X_test,y_test,X_train,y_train,n_neighbors,clf=None,namemethod=None,namefunction=None):\n",
    "    links,ind_posX,ind_negX = is_tomek(X_train,y_train,class_type=[-1.0])\n",
    "    #print(len(ind_posX))\n",
    "    new_W = weight\n",
    "    pos_index = np.where(y_train == 1)[0]\n",
    "    neg_index = np.where(y_train == -1)[0]\n",
    "    clf = Wsvm(C,new_W)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_predict = clf.predict(X_test)\n",
    "    specificity,sensitivity,gmean = Gmean(y_test,y_predict)\n",
    "    nn2 = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn2.fit(X_train)\n",
    "\n",
    "    # Mẫu âm bị phân loại sai -> Giảm trọng số của mẫu âm đó\n",
    "    neg_pred = clf.predict(X_train[neg_index])\n",
    "    idx_neg_wrong = np.where(neg_pred != -1.0)\n",
    "    new_W[idx_neg_wrong] =  new_W[idx_neg_wrong]*0.5 # giam manh (Co-Author*0.9)\n",
    "\n",
    "    # Tăng, giảm trọng số của các mẫu trong TLPs\n",
    "    # Trường hợp 1, 2, 3, 4\n",
    "    ind_nn_pos = [] # chứa chỉ số của các mẫu dương bị phân loại sai trong ind_posX\n",
    "    y_nn_pos = [] #chứa nhãn của k mẫu dữ liệu gần nhất với mẫu dương được xét\n",
    "    ind_nn_neg = [] # chứa chỉ số của các mẫu âm bị phân loại sai trong ind_negX\n",
    "    y_nn_neg = [] # chứa nhãn của k mẫu dữ liệu gần nhất với mẫu âm được xét\n",
    "\n",
    "    for ind,i in enumerate(ind_posX): \n",
    "        y_pred = clf.predict([X_train[i]])\n",
    "        if y_pred == -1.0: # dương bị phân loại sai (3,4)\n",
    "            ind_nn_pos.append(ind)                          \n",
    "            knn_X = (nn2.kneighbors([X_train[i]])[1]).tolist()  \n",
    "            for j in knn_X[0]:\n",
    "                y_nn_pos.append(y_train[j])    # gom nhãn láng giềng của X_train[i] bị dự đoán sai vào y_nn\n",
    "        else: # dương được phân loại đúng (1,2)\n",
    "            new_W[ind_negX[ind]] = new_W[ind_negX[ind]]/1.2 #Co-Author/1.05\n",
    "            new_W[i] = new_W[i]*1.2 #Co-Author*1.05\n",
    "            \n",
    "    ind_nn_pos = np.array(ind_nn_pos)\n",
    "    y_nn_pos = np.array(y_nn_pos)\n",
    "    if len(y_nn_pos)>0:\n",
    "        y_nn_pos = np.array_split(y_nn_pos, len(y_nn_pos)/n_neighbors) \n",
    "        for ind,i in enumerate(range(0,len(y_nn_pos))):   #\n",
    "            if 1 not in y_nn_pos[i][1:]:      # Nếu không có nhãn 1 xung quanh X_train[i] bị dự đoán sai => nhiễu dương -> giảm mạnh trọng số\n",
    "                new_W[ind_posX[ind_nn_pos[ind]]] = new_W[ind_posX[ind_nn_pos[ind]]]*0.5\n",
    "            else:\n",
    "                new_W[ind_negX[ind_nn_pos[ind]]] = new_W[ind_negX[ind_nn_pos[ind]]]/1.2\n",
    "                new_W[ind_posX[ind_nn_pos[ind]]] = new_W[ind_posX[ind_nn_pos[ind]]]*1.2\n",
    "\n",
    "    # Trường hợp 2\n",
    "\n",
    "    for ind,i in enumerate(ind_negX): \n",
    "        y_pred_neg = clf.predict([X_train[i]])  #\n",
    "        if y_pred_neg == 1.0: # âm bị phân loại sai\n",
    "            ind_nn_neg.append(ind)                          \n",
    "            knn_X = (nn2.kneighbors([X_train[i]])[1]).tolist()  \n",
    "            for j in knn_X[0]:\n",
    "                y_nn_neg.append(y_train[j])    # gom nhãn láng giềng của X_train[i] bị dự đoán sai vào y_nn_neg\n",
    "\n",
    "    ind_nn_neg = np.array(ind_nn_neg)\n",
    "    y_nn_neg = np.array(y_nn_neg)\n",
    "    if len(y_nn_neg)>0:\n",
    "        y_nn_neg = np.array_split(y_nn_neg, len(y_nn_neg)/n_neighbors) \n",
    "        for ind,i in enumerate(range(0,len(y_nn_neg))):   #\n",
    "            if -1 not in y_nn_neg[i][1:]:      # Nếu không có nhãn 1 xung quanh X_train[i] bị dự đoán sai => nhiễu âm -> giảm mạnh trọng số\n",
    "                new_W[ind_negX[ind_nn_neg[ind]]] = new_W[ind_negX[ind_nn_neg[ind]]]*0.5\n",
    "            else:\n",
    "                new_W[ind_negX[ind_nn_neg[ind]]] = new_W[ind_negX[ind_nn_neg[ind]]]/1.2\n",
    "                new_W[ind_posX[ind_nn_neg[ind]]] = new_W[ind_posX[ind_nn_neg[ind]]]*1.2\n",
    "\n",
    "\n",
    "    return new_W,gmean,sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_tomelinks_new1(C,weight,X_test,y_test,X_train,y_train,n_neighbors,arr_tlp,clf=None,namemethod=None,namefunction=None):\n",
    "    ro1 = 0.1\n",
    "    ro3 = 0.1\n",
    "    ro4 = 0.5\n",
    "    ro2 = 0.5\n",
    "    # links,ind_posX,ind_negX = is_tomek(X_train,y_train,class_type=[-1.0])\n",
    "    \n",
    "    #print(len(ind_posX))\n",
    "    new_W = weight\n",
    "    pos_index = np.where(y_train == 1)[0]\n",
    "    neg_index = np.where(y_train == -1)[0]\n",
    "    clf = Wsvm(C,new_W)\n",
    "    clf.fit(X_train, y_train)    \n",
    "    nn2 = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nn2.fit(X_train)\n",
    "\n",
    "    # Mẫu âm bị phân loại sai -> Giảm trọng số của mẫu âm đó\n",
    "    neg_pred = clf.predict(X_train[neg_index])\n",
    "    idx_neg_wrong = np.where(neg_pred != -1.0)\n",
    "    new_W[idx_neg_wrong] =  new_W[idx_neg_wrong]*0.5 # giam manh (Co-Author*0.9)\n",
    "\n",
    "    # Tăng, giảm trọng số của các mẫu trong TLPs\n",
    "    # Trường hợp 1, 2, 3, 4\n",
    "    ind_nn_pos = [] # chứa chỉ số của các mẫu dương bị phân loại sai trong ind_posX\n",
    "    y_nn_pos = [] #chứa nhãn của k mẫu dữ liệu gần nhất với mẫu dương được xét\n",
    "    ind_nn_neg = [] # chứa chỉ số của các mẫu âm bị phân loại sai trong ind_negX\n",
    "    y_nn_neg = [] # chứa nhãn của k mẫu dữ liệu gần nhất với mẫu âm được xét\n",
    "\n",
    "    for ind,i in enumerate(arr_tlp):\n",
    "        y_pred_pos = clf.predict([X_train[arr_tlp[ind][0]]]) #positive\n",
    "        y_pred_neg = clf.predict([X_train[arr_tlp[ind][1]]]) #negative\n",
    "        if (y_pred_pos == 1) and (y_pred_neg == 1): #dương dự đoán đúng, âm dự đoán sai\n",
    "            new_W[arr_tlp[ind][0]] = new_W[arr_tlp[ind][0]]*(1 + ro1) # tăng trọng số mẫu dương\n",
    "            new_W[arr_tlp[ind][1]] = new_W[arr_tlp[ind][1]]*(1 - ro1) # giảm trọng số mẫu âm\n",
    "            arr_tlp[ind][2] = 1\n",
    "\n",
    "            ind_nn_neg.append(ind)                          \n",
    "            knn_X = (nn2.kneighbors([X_train[arr_tlp[ind][1]]])[1]).tolist()  #Xem lại\n",
    "            for j in knn_X[0]:\n",
    "                y_nn_neg.append(y_train[j])    # gom nhãn láng giềng của X_train[i] bị dự đoán sai vào y_nn_neg\n",
    "\n",
    "        if (y_pred_pos == -1) and (y_pred_neg == -1): #âm dự đoán đúng, dương dự đoán sai\n",
    "            new_W[arr_tlp[ind][0]] = new_W[arr_tlp[ind][0]]*(1 + ro3) \n",
    "            new_W[arr_tlp[ind][1]] = new_W[arr_tlp[ind][1]]*(1 - ro3)\n",
    "            arr_tlp[ind][2] = 3\n",
    "\n",
    "            ind_nn_pos.append(ind)                          \n",
    "            knn_X = (nn2.kneighbors([X_train[arr_tlp[ind][0]]])[1]).tolist() #Xem lại \n",
    "            for j in knn_X[0]:\n",
    "                y_nn_pos.append(y_train[j])    # gom nhãn láng giềng của X_train[i] bị dự đoán sai vào y_nn\n",
    "\n",
    "    ind_nn_neg = np.array(ind_nn_neg)\n",
    "    y_nn_neg = np.array(y_nn_neg)\n",
    "    if len(y_nn_neg)>0:\n",
    "        y_nn_neg = np.array_split(y_nn_neg, len(y_nn_neg)/n_neighbors) \n",
    "        for ind,i in enumerate(range(0,len(y_nn_neg))):   #\n",
    "            if -1 not in y_nn_neg[i][1:]:      # Nếu không tồn tại nhãn -1 xung quanh X_train[i] bị dự đoán sai => nhiễu âm -> giảm mạnh trọng số\n",
    "                for a in arr_tlp[[ind_nn_neg[ind]]]:\n",
    "                    new_W[a[1]] = new_W[a[1]]*ro2\n",
    "                arr_tlp[ind][2] = 2\n",
    "\n",
    "    ind_nn_pos = np.array(ind_nn_pos)\n",
    "    y_nn_pos = np.array(y_nn_pos)\n",
    "    if len(y_nn_pos)>0:\n",
    "        y_nn_pos = np.array_split(y_nn_pos, len(y_nn_pos)/n_neighbors) \n",
    "        for ind,i in enumerate(range(0,len(y_nn_pos))):   #\n",
    "            if 1 not in y_nn_pos[i][1:]:      # Nếu không tồn tại nhãn 1 xung quanh X_train[i] bị dự đoán sai => nhiễu dương -> giảm mạnh trọng số\n",
    "                for a in arr_tlp[[ind_nn_pos[ind]]]:\n",
    "                    new_W[a[0]] = new_W[a[0]]*ro4\n",
    "                arr_tlp[ind][2] = 4\n",
    "\n",
    "    return new_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "<class 'numpy.ndarray'>\n",
      "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      " -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
      "<class 'numpy.ndarray'>\n",
      "[[183 118   0]\n",
      " [182 223   0]\n",
      " [210 227   0]\n",
      " [178 230   0]\n",
      " [200 239   0]\n",
      " [158 245   0]\n",
      " [157 252   0]\n",
      " [180 254   0]\n",
      " [205 314   0]]\n",
      "[-1.  1.  1.  1.  1. -1.  1.  1.  1.  1. -1.  1. -1.  1.  1.]\n",
      "[4 5 8]\n",
      "[0.58 0.6  0.48 0.5  0.59 0.73 0.76]\n",
      "[0.47 0.46 0.48 0.5  0.62 0.74 0.77]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X, y = Ecoli_Kfold.load_data()\n",
    "X = np.array(X)\n",
    "y_nn = []\n",
    "ind_nn = []\n",
    "arr_tlp = is_tomek_new(X, y, class_type=[-1.0])\n",
    "links, positive_ind, negative_ind = is_tomek(X, y, class_type=[-1.0])\n",
    "\n",
    "pos_index = np.where(y == 1)[0]\n",
    "neg_index = np.where(y == -1)[0]\n",
    "nn2 = NearestNeighbors(n_neighbors=5)\n",
    "nn2.fit(X)\n",
    "clf = KNeighborsClassifier(n_neighbors= 9)\n",
    "clf.fit(X,y)\n",
    "print(arr_tlp)\n",
    "ind_nn_neg = []\n",
    "y_nn_neg = []\n",
    "for ind,i in enumerate(arr_tlp):\n",
    "    y_pred_pos = clf.predict([X[arr_tlp[ind][0]]]) #positive\n",
    "    y_pred_neg = clf.predict([X[arr_tlp[ind][1]]]) #negative\n",
    "    # print(y_pred_pos)\n",
    "    # print(y_pred_neg)\n",
    "    # print(\"***\")\n",
    "    if (y_pred_pos == 1) and (y_pred_neg == 1): #dương dự đoán đúng\n",
    "        # new_W[arr_tlp[ind][0]] = new_W[arr_tlp[ind][0]]*(1 + ro1) \n",
    "        # new_W[arr_tlp[ind][1]] = new_W[arr_tlp[ind][1]]/(1 - ro1)\n",
    "        # arr_tlp[ind][2] = 1\n",
    "        ind_nn_neg.append(ind)                          \n",
    "        knn_X = (nn2.kneighbors([X[arr_tlp[ind][1]]])[1]).tolist()  \n",
    "        for j in knn_X[0]:\n",
    "            y_nn_neg.append(y[j])\n",
    "            # print(y_nn_neg)\n",
    "# print(arr_tlp)\n",
    "\n",
    "ind_nn_neg = np.array(ind_nn_neg)\n",
    "y_nn_neg = np.array(y_nn_neg)\n",
    "print(y_nn_neg)\n",
    "print(ind_nn_neg)\n",
    "if len(y_nn_neg)>0:\n",
    "    y_nn_neg = np.array_split(y_nn_neg, len(y_nn_neg)/5) \n",
    "    for ind,i in enumerate(range(0,len(y_nn_neg))):   #\n",
    "        if -1 not in y_nn_neg[i][1:]:      # Nếu không có nhãn 1 xung quanh X_train[i] bị dự đoán sai => nhiễu âm -> giảm mạnh trọng số\n",
    "            # new_W[arr_tlp[ind][1][[ind_nn_neg[ind]]]] = new_W[ind_negX[ind_nn_neg[ind]]]/ro2\n",
    "            # arr_tlp[ind][2] = 2\n",
    "            # print(\"element:\",arr_tlp[[ind_nn_neg[ind]]])\n",
    "            for a in arr_tlp[[ind_nn_neg[ind]]]:\n",
    "               print(X[a[1]])\n",
    "            # print([ind_nn_neg[ind]])\n",
    "            # new_W[ind_posX[ind_nn[ind]]]\n",
    "            \n",
    "            \n",
    "                \n",
    "# print(arr_tlp)\n",
    "# for ind,i in enumerate(negative_ind): \n",
    "#     y_pred = clf.predict([X[i]]) \n",
    "#     print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 3\n",
      "2 5\n"
     ]
    }
   ],
   "source": [
    "x = [ [0, 1], [2, 3], [4,5]]\n",
    "for ind, i in enumerate(x):\n",
    "    # for j in range(len(x[i])):\n",
    "        print(ind, x[ind][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfb(f,C,weight,namemethod,namefunction,T,X_test,y_test,X_train,y_train,n_neighbors,thamso1,thamso2): #loop find the best result\n",
    "    gmax = 0\n",
    "    tmax = 0\n",
    "    semax = 0\n",
    "    t_semax = 0\n",
    "    for i in range(0,1):\n",
    "        f.write(f\"\\t\\t Vong Lap thu: T = {i+1}\")\n",
    "        f.write(f\"\\n===================================================================================================================\")\n",
    "        f.write(f\"\\n\\n\\tFuzzy SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "        weight, gmeanfirst, SEfirst = data_tomelinks(f,C,weight,X_test,y_test,X_train,y_train,n_neighbors,clf=None,namemethod=namemethod,namefunction=namefunction)\n",
    "        #distribution_weight = fuzzy_weight(f,beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "        clf = Wsvm(C,weight)\n",
    "        #print(X_train)\n",
    "        #print(y_train)\n",
    "        clf.fit(X_train, y_train)\n",
    "        #print(clf.predict([X_test_val[5]]))\n",
    "        pred2 = clf.predict(X_test)    #X_val\n",
    "        sp,se,gmean = Gmean(y_test,pred2) #y_val\n",
    "        pred_all = clf.predict(X_test)    #X_val\n",
    "        sp_all,se_all,gmean_all = Gmean(y_test,pred_all)\n",
    "\n",
    "        if(gmeanfirst > gmax):\n",
    "            gmax = gmeanfirst\n",
    "            tmax = i+100\n",
    "        if (gmean_all > gmax):\n",
    "            gmax = gmean_all\n",
    "            tmax = i\n",
    "        if (SEfirst > semax):\n",
    "            semax = SEfirst\n",
    "            t_semax = i+100\n",
    "        if (se_all > semax):\n",
    "            semax = se_all\n",
    "            t_semax = i   \n",
    "        #metr(X_train,y_test_val,pred2,sp,se,gmean)\n",
    "        f.write(f\"\\n\\n\\t****** Danh gia tren tap Test:\\n\")\n",
    "        metr_text(f,X_train,y_test,pred_all,sp_all,se_all,gmean_all)\n",
    "        # if ((gmeanfirst - gmean) <= thamso1) or (gmeanfirst > thamso2):\n",
    "        #     f.write(\"\\n_____Gmean_ERROR!!!____\\n\")\n",
    "        #     print(\"\\n_____Gmean_ERROR!!!____\\n\")\n",
    "        #     return X_train, y_train\n",
    "        # else:\n",
    "        #     gmean = gmeanfirst\n",
    "        f.write(f\"\\n===================================================================================================================\\n\")\n",
    "    f.write(f\"\\nFuzzy SVM name_method = '{namemethod}',name_function = '{namefunction}'\")\n",
    "    f.write(f\"\\n*** T = {tmax}; K = {n_neighbors}; GmeanMax = {gmax:0.4f}\\n\")\n",
    "    f.write(f\"\\n*** T = {t_semax}; K = {n_neighbors}; SeMax = {semax:0.4f}\\n\")\n",
    "    f.write(f\"\\n===================================================================================================================\\n\")\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def compute_average_result(filepath,filename,time,times):\n",
    "    data = pd.read_csv(filepath)\n",
    "    sp_svm = se_svm = gm_svm = f1s_svm = acc_svm = auc_svm = 0\n",
    "    sp_wsvm = se_wsvm = gm_wsvm = f1s_wsvm = acc_wsvm = auc_wsvm = 0\n",
    "    sp_cen_lin = se_cen_lin = gm_cen_lin = f1s_cen_lin = acc_cen_lin = auc_cen_lin = 0\n",
    "    sp_cen_exp = se_cen_exp = gm_cen_exp = f1s_cen_exp = acc_cen_exp = auc_cen_exp = 0\n",
    "    sp_shp_lin = se_shp_lin = gm_shp_lin = f1s_shp_lin = acc_shp_lin = auc_shp_lin = 0\n",
    "    sp_shp_exp = se_shp_exp = gm_shp_exp = f1s_shp_exp = acc_shp_exp = auc_shp_exp = 0\n",
    "    sp_hyp_lin = se_hyp_lin = gm_hyp_lin = f1s_hyp_lin = acc_hyp_lin = auc_hyp_lin = 0\n",
    "    sp_hyp_exp = se_hyp_exp = gm_hyp_exp = f1s_hyp_exp = acc_hyp_exp = auc_hyp_exp = 0\n",
    "    sp_new1 = se_new1 = gm_new1 = f1s_new1 = acc_new1 = auc_new1 = 0\n",
    "    sp_new2 = se_new2 = gm_new2 = f1s_new2 = acc_new2 = auc_new2 = 0\n",
    "    sp_new3 = se_new3 = gm_new3 = f1s_new3 = acc_new3 = auc_new3 = 0\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "        # if(data['Times'][i] == '1'):\n",
    "        if (data['Name Method'][i] == 'SVM') and (data['Name Function'][i] == 'SVM'):\n",
    "            sp_svm = sp_svm + float(data['SP'][i])\n",
    "            se_svm = se_svm + float(data['SE'][i])\n",
    "            gm_svm = gm_svm + float(data['Gmean'][i])\n",
    "            f1s_svm = f1s_svm + float(data['F1 Score'][i])\n",
    "            acc_svm = acc_svm + float(data['Accuracy'][i])\n",
    "            auc_svm = auc_svm + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'WSVM') and (data['Name Function'][i] == 'WSVM'):\n",
    "            sp_wsvm = sp_wsvm + float(data['SP'][i])\n",
    "            se_wsvm = se_wsvm + float(data['SE'][i])\n",
    "            gm_wsvm = gm_wsvm + float(data['Gmean'][i])\n",
    "            f1s_wsvm = f1s_wsvm + float(data['F1 Score'][i])\n",
    "            acc_wsvm = acc_wsvm + float(data['Accuracy'][i])\n",
    "            auc_wsvm = auc_wsvm + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'own_class_center') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "            sp_cen_lin = sp_cen_lin + float(data['SP'][i])\n",
    "            se_cen_lin = se_cen_lin + float(data['SE'][i])\n",
    "            gm_cen_lin = gm_cen_lin + float(data['Gmean'][i])\n",
    "            f1s_cen_lin = f1s_cen_lin + float(data['F1 Score'][i])\n",
    "            acc_cen_lin = acc_cen_lin + float(data['Accuracy'][i])\n",
    "            auc_cen_lin = auc_cen_lin + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'own_class_center') and (data['Name Function'][i] == 'exp'):\n",
    "            sp_cen_exp = sp_cen_exp + float(data['SP'][i])\n",
    "            se_cen_exp = se_cen_exp+ float(data['SE'][i])\n",
    "            gm_cen_exp = gm_cen_exp + float(data['Gmean'][i])\n",
    "            f1s_cen_exp = f1s_cen_exp + float(data['F1 Score'][i])\n",
    "            acc_cen_exp = acc_cen_exp + float(data['Accuracy'][i])\n",
    "            auc_cen_exp = auc_cen_exp + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'estimated_hyper_lin') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "            sp_shp_lin = sp_shp_lin + float(data['SP'][i])\n",
    "            se_shp_lin = se_shp_lin + float(data['SE'][i])\n",
    "            gm_shp_lin = gm_shp_lin + float(data['Gmean'][i])\n",
    "            f1s_shp_lin = f1s_shp_lin + float(data['F1 Score'][i])\n",
    "            acc_shp_lin = acc_shp_lin + float(data['Accuracy'][i])\n",
    "            auc_shp_lin = auc_shp_lin + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'estimated_hyper_lin') and (data['Name Function'][i] == 'exp'):\n",
    "            sp_shp_exp = sp_shp_exp + float(data['SP'][i])\n",
    "            se_shp_exp = se_shp_exp+ float(data['SE'][i])\n",
    "            gm_shp_exp = gm_shp_exp + float(data['Gmean'][i])\n",
    "            f1s_shp_exp = f1s_shp_exp + float(data['F1 Score'][i])\n",
    "            acc_shp_exp = acc_shp_exp + float(data['Accuracy'][i])\n",
    "            auc_shp_exp = auc_shp_exp + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'actual_hyper_lin') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "            sp_hyp_lin = sp_hyp_lin + float(data['SP'][i])\n",
    "            se_hyp_lin = se_hyp_lin + float(data['SE'][i])\n",
    "            gm_hyp_lin = gm_hyp_lin + float(data['Gmean'][i])\n",
    "            f1s_hyp_lin = f1s_hyp_lin + float(data['F1 Score'][i])\n",
    "            acc_hyp_lin = acc_hyp_lin + float(data['Accuracy'][i])\n",
    "            auc_hyp_lin = auc_hyp_lin + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'actual_hyper_lin') and (data['Name Function'][i] == 'exp'):\n",
    "            sp_hyp_exp = sp_hyp_exp + float(data['SP'][i])\n",
    "            se_hyp_exp = se_hyp_exp + float(data['SE'][i])\n",
    "            gm_hyp_exp = gm_hyp_exp + float(data['Gmean'][i])\n",
    "            f1s_hyp_exp = f1s_hyp_exp + float(data['F1 Score'][i])\n",
    "            acc_hyp_exp = acc_hyp_exp + float(data['Accuracy'][i])\n",
    "            auc_hyp_exp = auc_hyp_exp + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new'):\n",
    "            sp_new1 = sp_new1 + float(data['SP'][i])\n",
    "            se_new1 = se_new1 + float(data['SE'][i])\n",
    "            gm_new1 = gm_new1 + float(data['Gmean'][i])\n",
    "            f1s_new1 = f1s_new1 + float(data['F1 Score'][i])\n",
    "            acc_new1 = acc_new1 + float(data['Accuracy'][i])\n",
    "            auc_new1 = auc_new1 + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new_v1'):\n",
    "            sp_new2 = sp_new2 + float(data['SP'][i])\n",
    "            se_new2 = se_new2 + float(data['SE'][i])\n",
    "            gm_new2 = gm_new2 + float(data['Gmean'][i])\n",
    "            f1s_new2 = f1s_new2 + float(data['F1 Score'][i])\n",
    "            acc_new2 = acc_new2 + float(data['Accuracy'][i])\n",
    "            auc_new2 = auc_new2 + float(data['AUC'][i])\n",
    "        elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new_v2'):\n",
    "            sp_new3= sp_new3 + float(data['SP'][i])\n",
    "            se_new3 = se_new3 + float(data['SE'][i])\n",
    "            gm_new3 = gm_new3 + float(data['Gmean'][i])\n",
    "            f1s_new3 = f1s_new3 + float(data['F1 Score'][i])\n",
    "            acc_new3 = acc_new3 + float(data['Accuracy'][i])\n",
    "            auc_new3 = auc_new3 + float(data['AUC'][i])\n",
    "        else:\n",
    "            print(\"end\")\n",
    "    header = ['Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC']\n",
    "    data = [['SVM','SVM',sp_svm/(5*times),se_svm/(5*times),gm_svm/(5*times),f1s_svm/(5*times),acc_svm/(5*times),auc_svm/(5*times)],\n",
    "            ['WSVM','WSVM',sp_wsvm/(5*times),se_wsvm/(5*times),gm_wsvm/(5*times),f1s_wsvm/(5*times),acc_wsvm/(5*times),auc_wsvm/(5*times)],\n",
    "            ['own_class_center','lin_center_own',sp_cen_lin/(5*times),se_cen_lin/(5*times),gm_cen_lin/(5*times),f1s_cen_lin/(5*times),acc_cen_lin/(5*times),auc_cen_lin/(5*times)],\n",
    "            ['own_class_center','exp',sp_cen_exp/(5*times),se_cen_exp/(5*times),gm_cen_exp/(5*times),f1s_cen_exp/(5*times),acc_cen_exp/(5*times),auc_cen_exp/(5*times)],\n",
    "            ['estimated_hyper_lin','lin_center_own',sp_shp_lin /(5*times),se_shp_lin/(5*times),gm_shp_lin/(5*times),f1s_shp_lin/(5*times),acc_shp_lin/(5*times),auc_shp_lin /(5*times)],\n",
    "            ['estimated_hyper_lin','exp',sp_shp_exp/(5*times),se_shp_exp /(5*times),gm_shp_exp/(5*times),f1s_shp_exp/(5*times),acc_shp_exp/(5*times),auc_shp_exp/(5*times)],\n",
    "            ['actual_hyper_lin','lin_center_own',sp_hyp_lin/(5*times),se_hyp_lin/(5*times),gm_hyp_lin/(5*times),f1s_hyp_lin/(5*times),acc_hyp_lin/(5*times),auc_hyp_lin/(5*times)],\n",
    "            ['actual_hyper_lin','exp',sp_hyp_exp/(5*times),se_hyp_exp/(5*times),gm_hyp_exp/(5*times),f1s_hyp_exp/(5*times),acc_hyp_exp/(5*times),auc_hyp_exp/(5*times)],\n",
    "            ['distance_center_own_opposite_tam','func_own_opp_new',sp_new1/(5*times),se_new1/(5*times),gm_new1/(5*times),f1s_new1/(5*times),acc_new1/(5*times),auc_new1/(5*times)],\n",
    "            ['distance_center_own_opposite_tam','func_own_opp_new_v1',sp_new2/(5*times),se_new2/(5*times),gm_new2/(5*times),f1s_new2/(5*times),acc_new2/(5*times),auc_new2/(5*times)],\n",
    "            ['distance_center_own_opposite_tam','func_own_opp_new_v2',sp_new3/(5*times),se_new3/(5*times),gm_new3/(5*times),f1s_new3/(5*times),acc_new3/(5*times),auc_new3/(5*times)]]\n",
    "\n",
    "    with open(f'./Experiment/Data_{filename}_{time}_Average.csv', 'a', encoding='UTF8', newline='') as f4:\n",
    "        writer = csv.writer(f4)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own_class_center lin_center_own\n",
      "own_class_center exp\n",
      "estimated_hyper_lin lin_center_own\n",
      "estimated_hyper_lin exp\n",
      "actual_hyper_lin lin_center_own\n",
      "actual_hyper_lin exp\n",
      "distance_center_own_opposite_tam func_own_opp_new\n",
      "distance_center_own_opposite_tam func_own_opp_new_v1\n",
      "distance_center_own_opposite_tam func_own_opp_new_v2\n"
     ]
    }
   ],
   "source": [
    "name_method =[\"own_class_center\",\"estimated_hyper_lin\",\"actual_hyper_lin\",\"distance_center_own_opposite_tam\"]\n",
    "name_function = [\"lin_center_own\",\"exp\",\"func_own_opp_new\", \"func_own_opp_new_v1\", \"func_own_opp_new_v2\"]\n",
    "for namemethod in name_method:\n",
    "    for namefunction in name_function:\n",
    "        if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "            continue\n",
    "        elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "            continue\n",
    "        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "            continue\n",
    "        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "            continue\n",
    "        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "            continue\n",
    "        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "            continue\n",
    "        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "            continue\n",
    "        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "            continue\n",
    "        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "            continue\n",
    "        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "            continue\n",
    "        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "            continue\n",
    "        # elif namemethod == \"distance_center_own_opposite_tam\" and namefunction == \"lin\":\n",
    "        #     continue\n",
    "        # elif namemethod == \"own_class_center\" and namefunction == \"lin\":\n",
    "        #     continue\n",
    "        # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"lin\":\n",
    "        #     continue\n",
    "        # elif namemethod == \"actual_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "        #    continue\n",
    "        # elif namemethod == \"own_class_center\" and namefunction == \"lin_center_own\":\n",
    "        #     continue \n",
    "        # elif namemethod == \"own_class_center\" and namefunction == \"exp\":\n",
    "        #     continue \n",
    "        # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "        #     continue \n",
    "        # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"exp\":\n",
    "        #     continue \n",
    "        # elif namemethod == \"actual_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "        #     continue\n",
    "        # elif namemethod == \"actual_hyper_lin\" and namefunction == \"exp\":\n",
    "        #     continue\n",
    "        else:\n",
    "            print(namemethod, namefunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 100\n",
    "\n",
    "# thamso1 = 1\n",
    "# thamso2 = 1\n",
    "# T = 20\n",
    "# N = 1\n",
    "# n_neighbor = 5\n",
    "# test_size = [0.2]\n",
    "# testsize_val = 0.2\n",
    "# K_big = 5\n",
    "# K_small = 5\n",
    "# new_rate = [1/3]\n",
    "# # data = [Co_Author, Abanole, Ecoli, Ecloli1, Ecoli3, Glass1, Glass4, Haberman, Waveform, New_thyroid2, Page_blocks,\n",
    "# #             Pima_Indians_Diabetes, Satimage, Transfusion, Yeast]\n",
    "\n",
    "# #Haberman dataset\n",
    "# # dataset = Haberman_KFold\n",
    "# # beta_center, beta_estimate, beta_actual = 1, 1, 0.6 # !!!!!!! Beta with Dataset, change Data please change Beta !!!!!!!!\n",
    "\n",
    "# # Ecoli dataset\n",
    "# # dataset = Ecoli_Kfold\n",
    "# # beta_center, beta_estimate, beta_actual = 0.3, 0.6, 0.7\n",
    "# dataset = Co_Author_TestSize\n",
    "# #Co-Author dataset\n",
    "\n",
    "# beta_center, beta_estimate, beta_actual = 0.3, 0.6, 0.7\n",
    "\n",
    "# name_method =[\"own_class_center\",\"estimated_hyper_lin\",\"actual_hyper_lin\",\"distance_center_own_opposite_tam\"]\n",
    "# #name_method =[\"own_class_center_divided\"]\n",
    "# name_function = [\"lin_center_own\",\"exp\",\"func_own_opp_new\", \"func_own_opp_new_v1\"]\n",
    "\n",
    "# time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "# filepath = \"./text_script\"\n",
    "\n",
    "# #svc lib\n",
    "# svc = SVC(probability=True, kernel='linear')\n",
    "# #svm scratch\n",
    "# svm_scr = Svm(C)\n",
    "# #W.svm\n",
    "# # for n in range(0,N):\n",
    "# #     print(\"Lan boc: \",n+1)\n",
    "#     # for dataset in data:\n",
    "# filename = (str(dataset).split(\"\\\\\")[-1]).split(\".\")[0]\n",
    "# f = open(f\"{filepath}/Data_{filename}_{time}_Detail.txt\", \"w\")\n",
    "# f.write(f\"\\nC = {C}, thamso1 = {thamso1}, thamso2 = {thamso2}, T = {T}, n_neighbors = {n_neighbor}  \\n\")\n",
    "# f.write(f\"\\n\\n\\tUSING DATASET : {filename}\\n\")\n",
    "\n",
    "# f2 = open(f\"{filepath}/Data_{filename}_{time}_Main.txt\", \"w\")\n",
    "# f2.write(f\"\\nC = {C}, thamso1 = {thamso1}, thamso2 = {thamso2}, T = {T}, n_neighbors = {n_neighbor}  \\n\")\n",
    "# f2.write(f\"\\n\\n\\tUSING DATASET : {filename}\\n\")\n",
    "    \n",
    "\n",
    "# print(f\"\\n\\tUSING DATASET : {filename}\\n\")\n",
    "# for testsize in test_size:\n",
    "#     for newrate in new_rate:\n",
    "#         X_train, y_train, X_test, y_test = dataset.load_data(test_size=testsize)\n",
    "#         # X, y = dataset.load_data()\n",
    "#         # print(X.shape)\n",
    "#         # kfold_validation = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "#         header = ['Test Size','Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC','Ma tran nham lan']\n",
    "#         data = []\n",
    "#         with open(f'./Experiment/Data_{filename}_{time}_Main.csv', 'a', encoding='UTF8', newline='') as f3:\n",
    "#             writer = csv.writer(f3)\n",
    "#             writer.writerow(header)\n",
    "#             # fold = 1\n",
    "#             # for train_index, test_index in kfold_validation.split(X,y):\n",
    "                \n",
    "#                 # X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "#                 # X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "#                 # print(X_test.shape)\n",
    "#                 # #Scalling Data\n",
    "#                 # sc_X = StandardScaler()\n",
    "#                 # X_train = sc_X.fit_transform(X_train)\n",
    "#                 # X_test = sc_X.transform(X_test)\n",
    "#                 # y_train = np.array(y_train)\n",
    "\n",
    "#                 # print(type(X_train), type(y_train))\n",
    "\n",
    "#                 #print(f\"\\t======== TestSize: {testsize} ========\")\n",
    "#                 #print(\"So luong sample nguyen ban ban dau: \",len(X_train)+len(X_test_val)+len(X_test))\n",
    "\n",
    "#                 # f.write(f\"\\n\\n\\t======== TestSize: {testsize} ========\\n\\n\")\n",
    "#             f.write(\"\\n\\n\\t====== NOT USING TOMEKLINKS ========== \\n\")\n",
    "\n",
    "#             # f2.write(f\"\\n\\n\\t======== TestSize: {testsize} ========\\n\\n\")\n",
    "#             f2.write(\"\\n\\n\\t====== NOT USING TOMEKLINKS ========== \\n\")\n",
    "\n",
    "#             #Svm library\n",
    "#             f.write(\"\\n\\n\\tSVM LIBRARY starting...\\n\")\n",
    "#             f2.write(\"\\n\\n\\tSVM LIBRARY starting...\\n\")\n",
    "#             #print(\"SVM LIBRARY starting...\\n\")\n",
    "#             test_pred = svm_lib(X_train, y_train,X_test)\n",
    "#             sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#             #metr(X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             metr_text(f,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             metr_text(f2,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             name1 = 'SVM'\n",
    "#             name2 = 'SVM'\n",
    "#             data.append([testsize,name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "            \n",
    "#             # #Svm scratch\n",
    "#             # f.write(\"\\n\\n\\tSVM starting...\\n\")\n",
    "#             # f2.write(\"\\n\\n\\tSVM starting...\\n\")\n",
    "#             # print(\"SVM starting...\\n\")\n",
    "#             # test_pred = svm(C,X_train, y_train,X_test)\n",
    "#             # sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#             # #metr(X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             # metr_text(f,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             # metr_text(f2,X_train,y_test,test_pred,sp,se,gmean)\n",
    "\n",
    "#             #Wsvm\n",
    "#             f.write(\"\\n\\n\\tW.SVM starting...\\n\")\n",
    "#             f2.write(\"\\n\\n\\tW.SVM starting...\\n\")\n",
    "#             print(\"W.SVM starting...\\n\")\n",
    "#             N, d = X_train.shape\n",
    "#             distribution_weight = np.ones(N)\n",
    "\n",
    "#             #header = ['Fold','Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC','Ma tran nham lan']\n",
    "#             #data1 = []\n",
    "#             #with open('Result_C.csv', 'w', encoding='UTF8', newline='') as f4:\n",
    "#                 ##writer1.writerow(header)\n",
    "#             # for i in range(1,15):\n",
    "#             #     C = 2**i\n",
    "#             test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "#             sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#             #     #metr(X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             metr_text(f,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             metr_text(f2,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             name1 = 'WSVM'\n",
    "#             name2 = 'WSVM'\n",
    "#             #     data.append([fold,name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "#             data.append([testsize,name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "#             # writer.writerows(data)\n",
    "\n",
    "#             #FuzyyWsvm\n",
    "#             for namemethod in name_method:\n",
    "#                 for namefunction in name_function:\n",
    "#                     if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "#                         continue\n",
    "#                     elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                         continue\n",
    "#                     elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                         continue\n",
    "#                     else:\n",
    "#                         f.write(f\"\\n\\n\\tFuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#                         f2.write(f\"\\n\\n\\tFuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#                         print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#                         distribution_weight = fuzzy_weight(f,beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#                         __ = fuzzy_weight(f2,beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#                         test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "#                         sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#                         metr_text(f,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#                         metr_text(f2,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#                         data.append([testsize,namemethod,namefunction,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "#             # # f.write(f\"\\n\\n\\t======== TestSize: {testsize} ========\\n\\n\")\n",
    "#             # f.write(\"\\n\\n\\t====== USING TOMEKLINKS ========== \\n\")\n",
    "\n",
    "#             # # f2.write(f\"\\n\\n\\t======== TestSize: {testsize} ========\\n\\n\")\n",
    "#             # f2.write(\"\\n\\n\\t====== USING TOMEKLINKS ========== \\n\")\n",
    "\n",
    "#             # arr_tlp = is_tomek_new(X_train, y_train, class_type = [-1])\n",
    "#             # #FuzyyWsvm\n",
    "#             # for namemethod in name_method:\n",
    "#             #     for namefunction in name_function:\n",
    "#             #         if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "#             #             continue\n",
    "#             #         elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "#             #             continue\n",
    "#             #         elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "#             #             continue\n",
    "#             #         elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#             #             continue\n",
    "#             #         elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#             #             continue\n",
    "#             #         # elif namemethod == \"distance_center_own_opposite_tam\" and namefunction == \"lin\":\n",
    "#             #         #     continue\n",
    "#             #         # elif namemethod == \"own_class_center\" and namefunction == \"lin\":\n",
    "#             #         #     continue\n",
    "#             #         # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"lin\":\n",
    "#             #         #     continue\n",
    "#             #         # elif namemethod == \"actual_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "#             #         # #     continue\n",
    "#             #         # elif namemethod == \"own_class_center\" and namefunction == \"lin_center_own\":\n",
    "#             #         #     continue \n",
    "#             #         # elif namemethod == \"own_class_center\" and namefunction == \"exp\":\n",
    "#             #         #     continue \n",
    "#             #         # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "#             #         #     continue \n",
    "#             #         # elif namemethod == \"estimated_hyper_lin\" and namefunction == \"exp\":\n",
    "#             #         #     continue \n",
    "#             #         # elif namemethod == \"actual_hyper_lin\" and namefunction == \"lin_center_own\":\n",
    "#             #         #     continue\n",
    "#             #         # elif namemethod == \"actual_hyper_lin\" and namefunction == \"exp\":\n",
    "#             #         #     continue\n",
    "#             #         else:\n",
    "#             #             f.write(f\"\\n\\n\\tFuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#             #             f2.write(f\"\\n\\n\\tFuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#             #             print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#             #             distribution_weight = fuzzy_weight(f,beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#             #             __ = fuzzy_weight(f2,beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#             #             # fold_a = f\"{fold}_a\"\n",
    "                        \n",
    "#             #             for i in range(0,T):\n",
    "#             #                 # new_W = lfb(f,C,distribution_weight,namemethod,namefunction,T,X_test,y_test,X_train,y_train,n_neighbor,thamso1,thamso2)\n",
    "#             #                 new_W = data_tomelinks_new1(f,C,distribution_weight,X_test,y_test,X_train,y_train,n_neighbor,arr_tlp,clf=None,namemethod=namemethod,namefunction=namefunction)\n",
    "#             #                 test_pred = wsvm(C,X_train, y_train, X_test, new_W)\n",
    "#             #                 sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#             #                 #metr(X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             #                 metr_text(f,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             #                 metr_text(f2,X_train,y_test,test_pred,sp,se,gmean)\n",
    "#             #                 # fold_as=f\"{fold_a}_{i+1}\"\n",
    "#             #                 data.append([testsize,namemethod,namefunction,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "#             # # fold = fold + 1\n",
    "#             writer.writerows(data)\n",
    "#         f.write(\"\\n===================================================================================\\n\")\n",
    "#         f.close()\n",
    "#         f2.write(\"\\n===================================================================================\\n\")\n",
    "#         f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 100\n",
    "# thamso1 = 1\n",
    "# thamso2 = 1\n",
    "# T = 5\n",
    "# N = 5\n",
    "# n_neighbor = 5\n",
    "# test_size = [0.2,0.3,0.4]\n",
    "# testsize_val = 0.2\n",
    "# K_big = 5\n",
    "# K_small = 5\n",
    "# new_rate = [1/5, 1/7, 1/9]\n",
    "# # data = [Co_Author, Abanole, Ecoli, Ecloli1, Ecoli3, Glass1, Glass4, Haberman, Waveform, New_thyroid2, Page_blocks,\n",
    "# #             Pima_Indians_Diabetes, Satimage, Transfusion, Yeast]\n",
    "\n",
    "# #Haberman dataset\n",
    "# # dataset = Haberman_KFold\n",
    "# # beta_center, beta_estimate, beta_actual = 1, 1, 0.6 # !!!!!!! Beta with Dataset, change Data please change Beta !!!!!!!!\n",
    "\n",
    "# # Ecoli dataset\n",
    "# # dataset = Ecoli_Kfold\n",
    "# # beta_center, beta_estimate, beta_actual = 0.3, 0.6, 0.7\n",
    "\n",
    "# # dataset = Pima\n",
    "# # beta_center, beta_estimate, beta_actual = 0.5, 1, 0.5\n",
    "\n",
    "# # dataset = Transfution_Kfold\n",
    "# # beta_center, beta_estimate, beta_actual = 0.5, 0.8, 0.1\n",
    "# from Processing_Data import Co_Author_50_250\n",
    "# dataset = Co_Author_50_250\n",
    "# beta_center, beta_estimate, beta_actual = 0.5, 0.8, 0.1\n",
    "\n",
    "# name_method =[\"own_class_center\",\"estimated_hyper_lin\",\"actual_hyper_lin\",\"distance_center_own_opposite_tam\"]\n",
    "# name_function = [\"lin_center_own\",\"exp\",\"func_own_opp_new\", \"func_own_opp_new_v1\",\"func_own_opp_new_v2\"]\n",
    "\n",
    "# time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "# filename = (str(dataset).split(\"\\\\\")[-1]).split(\".\")[0]\n",
    "# filepath = f'./Experiment/Data_{filename}_Full.csv'\n",
    "\n",
    "# #svc lib\n",
    "# svc = SVC(probability=True, kernel='linear')\n",
    "# #svm scratch\n",
    "# svm_scr = Svm(C)\n",
    "# #W.svm\n",
    "# for n in range(0,N):\n",
    "#     print(\"Lan boc: \",n+1)\n",
    "#     for ir in new_rate:\n",
    "#         X, y = dataset.load_data(ir)\n",
    "#         print(X.shape)\n",
    "#         kfold_validation = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "#         header = ['Times','IR','Fold','Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC','Ma tran nham lan']\n",
    "#         data = []\n",
    "#         with open(f'./Experiment/Data_{filename}_{time}_Full.csv', 'a', encoding='UTF8', newline='') as f3:\n",
    "#             writer = csv.writer(f3)\n",
    "#             writer.writerow(header)\n",
    "#             fold = 1\n",
    "#             for train_index, test_index in kfold_validation.split(X,y):\n",
    "                \n",
    "#                 # X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "#                 # X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "#                 X_train, y_train = X[train_index], y[train_index]\n",
    "#                 X_test, y_test = X[test_index], y[test_index]\n",
    "#                 print(X_test.shape)\n",
    "                \n",
    "#                 #Scalling Data\n",
    "#                 sc_X = StandardScaler()\n",
    "#                 X_train = sc_X.fit_transform(X_train)\n",
    "#                 X_test = sc_X.transform(X_test)\n",
    "#                 y_train = np.array(y_train)\n",
    "\n",
    "#                 # NOT USING TOMEK LINKS\n",
    "\n",
    "#                 #Svm library\n",
    "#                 print(\"SVM LIBRARY starting...\\n\")\n",
    "#                 test_pred = svm_lib(X_train, y_train,X_test)\n",
    "#                 sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#                 name1 = 'SVM'\n",
    "#                 name2 = 'SVM'\n",
    "#                 data.append([n,ir,fold,name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "                \n",
    "#                 #Wsvm\n",
    "#                 print(\"W.SVM starting...\\n\")\n",
    "#                 N, d = X_train.shape\n",
    "#                 distribution_weight = np.ones(N)\n",
    "#                 test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "#                 sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#                 name1 = 'WSVM'\n",
    "#                 name2 = 'WSVM'\n",
    "#                 data.append([n,ir,fold,name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "#                 #FuzyyWsvm\n",
    "#                 for namemethod in name_method:\n",
    "#                     for namefunction in name_function:\n",
    "#                         if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "#                             continue\n",
    "#                         elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                             continue\n",
    "#                         elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "#                             continue\n",
    "#                         else:\n",
    "#                             print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "#                             distribution_weight = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#                             __ = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "#                             test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "#                             sp,se,gmean = Gmean(y_test,test_pred)\n",
    "#                             data.append([n,ir,fold,namemethod,namefunction,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "#                 fold = fold + 1\n",
    "#             writer.writerows(data)\n",
    "# # compute_average_result(filepath,filename,time,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# K-FOLD SCRIPT FOR CO-AUTHOR DATASETS ##############################\n",
    "C = 100\n",
    "T = 1\n",
    "N = 3\n",
    "n_neighbor = 5\n",
    "test_size = [0.2,0.3,0.4]\n",
    "K_big = 5\n",
    "K_small = 5\n",
    "new_rate = [1/5, 1/7, 1/9]\n",
    "\n",
    "from Processing_Data import Co_Author_50_250, Co_Author_250_750, Co_Author_100_900, Co_Author_100_700, Co_Author_100_500, Co_Author_50_350, Co_Author_200_1000, Co_Author_200_1000_1, Co_Author_300_1500\n",
    "dataset = Co_Author_200_1000\n",
    "beta_center, beta_estimate, beta_actual = 0.5, 0.8, 0.1\n",
    "\n",
    "name_method =[\"own_class_center\",\"estimated_hyper_lin\",\"actual_hyper_lin\",\"distance_center_own_opposite_tam\"]\n",
    "name_function = [\"lin_center_own\",\"exp\",\"func_own_opp_new\", \"func_own_opp_new_v1\",\"func_own_opp_new_v2\"]\n",
    "\n",
    "time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "filename = (str(dataset).split(\"\\\\\")[-1]).split(\".\")[0]\n",
    "\n",
    "#W.svm\n",
    "for n in range(0,N):\n",
    "    print(\"Lan boc: \",n+1)\n",
    "    X, y = dataset.load_data()\n",
    "    print(X.shape)\n",
    "    kfold_validation = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    header = ['Times','Fold','T','Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC','Ma tran nham lan']\n",
    "    data = []\n",
    "    with open(f'./Experiment/KFold_Data_{filename}_{time}.csv', 'a', encoding='UTF8', newline='') as f3:\n",
    "        writer = csv.writer(f3)\n",
    "        writer.writerow(header)\n",
    "        fold = 1\n",
    "        for train_index, test_index in kfold_validation.split(X,y):\n",
    "            \n",
    "            # X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "            # X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "            X_train, y_train = X[train_index], y[train_index]\n",
    "            X_test, y_test = X[test_index], y[test_index]\n",
    "            print(X_test.shape)\n",
    "            \n",
    "            #Scalling Data\n",
    "            sc_X = StandardScaler()\n",
    "            X_train = sc_X.fit_transform(X_train)\n",
    "            X_test = sc_X.transform(X_test)\n",
    "            y_train = np.array(y_train)\n",
    "\n",
    "            # NORMAL\n",
    "\n",
    "            #Svm library\n",
    "            print(\"SVM LIBRARY starting...\\n\")\n",
    "            test_pred = svm_lib(X_train, y_train,X_test)\n",
    "            sp,se,gmean = Gmean(y_test,test_pred)\n",
    "            name1 = 'SVM'\n",
    "            name2 = 'SVM'\n",
    "            data.append([n,fold,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "            \n",
    "            #Wsvm\n",
    "            print(\"W.SVM starting...\\n\")\n",
    "            N, d = X_train.shape\n",
    "            distribution_weight = np.ones(N)\n",
    "            test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "            sp,se,gmean = Gmean(y_test,test_pred)\n",
    "            name1 = 'WSVM'\n",
    "            name2 = 'WSVM'\n",
    "            data.append([n,fold,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "            #FuzyyWsvm\n",
    "            for namemethod in name_method:\n",
    "                for namefunction in name_function:\n",
    "                    if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "                        continue\n",
    "                    elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "                        distribution_weight = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                        __ = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                        test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "                        sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                        data.append([n,fold,\"None\",namemethod,namefunction,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "            ############ USING AFW-CIL METHOD #########################\n",
    "\n",
    "            arr_tlp = is_tomek_new(X_train, y_train, class_type = [-1.0])\n",
    "            #FuzyyWsvm\n",
    "            for namemethod in name_method:\n",
    "                for namefunction in name_function:\n",
    "                    if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "                        continue\n",
    "                    elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                        continue\n",
    "                    elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"===========USING AFW-CIL=====================\")\n",
    "                        print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "                        distribution_weight = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                        __ = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                        \n",
    "                        for i in range(0,T):\n",
    "                            new_W = data_tomelinks_new1(C,distribution_weight,X_test,y_test,X_train,y_train,n_neighbor,arr_tlp,clf=None,namemethod=namemethod,namefunction=namefunction)\n",
    "                            test_pred = wsvm(C,X_train, y_train, X_test, new_W)\n",
    "                            sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                            data.append([n,fold,i+1,namemethod,namefunction,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "            fold = fold + 1\n",
    "        writer.writerows(data)\n",
    "    \n",
    "# compute_average_result(filepath,filename,time,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### TEST-SIZE SCRIPT FOR CO-AUTHOR DATASETS #################################\n",
    "C = 100\n",
    "T = 20\n",
    "N = 1\n",
    "n_neighbor = 5\n",
    "test_size = [0.2]\n",
    "new_rate = [1/3]\n",
    "\n",
    "dataset = Ecoli_All\n",
    "beta_center, beta_estimate, beta_actual = 0.3, 0.6, 0.7\n",
    "\n",
    "name_method =[\"own_class_center\",\"estimated_hyper_lin\",\"actual_hyper_lin\",\"distance_center_own_opposite_tam\"]\n",
    "name_function = [\"lin_center_own\",\"exp\",\"func_own_opp_new\", \"func_own_opp_new_v1\",\"func_own_opp_new_v2\"]\n",
    "time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "filename = (str(dataset).split(\"\\\\\")[-1]).split(\".\")[0]\n",
    "\n",
    "\n",
    "for n in range(0,N):\n",
    "    print(\"Lần bốc thứ: \",n+1)\n",
    "    header = ['Lan boc','Test Size','Rate','T','Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC','Ma tran nham lan']\n",
    "    data = []\n",
    "    for testsize in test_size:\n",
    "        for newrate in new_rate:\n",
    "            X_train, y_train, X_test, y_test = dataset.load_data()\n",
    "            with open(f'./Experiment/TestSize_Data_{filename}_{time}.csv', 'a', encoding='UTF8', newline='') as f3:\n",
    "                writer = csv.writer(f3)\n",
    "                writer.writerow(header)\n",
    "                \n",
    "                #Scalling Data\n",
    "                sc_X = StandardScaler()\n",
    "                X_train = sc_X.fit_transform(X_train)\n",
    "                X_test = sc_X.transform(X_test)\n",
    "                y_train = np.array(y_train)\n",
    "\n",
    "                # NORMAL\n",
    "\n",
    "                #Svm library\n",
    "                print(\"SVM LIBRARY starting...\\n\")\n",
    "                test_pred = svm_lib(X_train, y_train,X_test)\n",
    "                sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                name1 = 'SVM'\n",
    "                name2 = 'SVM'\n",
    "                data.append([n+1,testsize,newrate,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "                \n",
    "                #Wsvm\n",
    "                print(\"W.SVM starting...\\n\")\n",
    "                N, d = X_train.shape\n",
    "                distribution_weight = np.ones(N)\n",
    "                test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "                sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                name1 = 'WSVM'\n",
    "                name2 = 'WSVM'\n",
    "                data.append([n+1,testsize,newrate,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "                #FuzyyWsvm\n",
    "                for namemethod in name_method:\n",
    "                    for namefunction in name_function:\n",
    "                        if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "                            continue\n",
    "                        elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "                            distribution_weight = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                            __ = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                            test_pred = wsvm(C,X_train, y_train, X_test, distribution_weight)\n",
    "                            sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                            data.append([n+1,testsize,newrate,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "\n",
    "                ############ USING AFW-CIL METHOD #########################\n",
    "                arr_tlp = is_tomek_new(X_train, y_train, class_type = [-1.0])\n",
    "                #FuzyyWsvm\n",
    "                for namemethod in name_method:\n",
    "                    for namefunction in name_function:\n",
    "                        if namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"lin_center_own\":\n",
    "                            continue\n",
    "                        elif namemethod ==\"distance_center_own_opposite_tam\" and namefunction ==\"exp\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v1\":\n",
    "                            continue\n",
    "                        elif namemethod == \"own_class_center\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        elif namemethod == \"estimated_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        elif namemethod == \"actual_hyper_lin\" and namefunction == \"func_own_opp_new_v2\":\n",
    "                            continue\n",
    "                        else:\n",
    "                            print(\"===========USING AFW-CIL=====================\")\n",
    "                            print(f\"Fuzzy W.SVM name_method = '{namemethod}',name_function = '{namefunction}' starting...\\n\")\n",
    "                            distribution_weight = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                            __ = fuzzy_weight(beta_center, beta_estimate, beta_actual,X_train, y_train,namemethod,namefunction)\n",
    "                            \n",
    "                            for i in range(0,T):\n",
    "                                new_W = data_tomelinks_new1(C,distribution_weight,X_test,y_test,X_train,y_train,n_neighbor,arr_tlp,clf=None,namemethod=namemethod,namefunction=namefunction)\n",
    "                                test_pred = wsvm(C,X_train, y_train, X_test, new_W)\n",
    "                                sp,se,gmean = Gmean(y_test,test_pred)\n",
    "                                data.append([n+1,testsize,newrate,\"None\",name1,name2,sp,se,gmean,f1_score(y_test, test_pred),accuracy_score(y_test,test_pred),roc_auc_score(y_test, test_pred),str(confusion_matrix(y_test, test_pred))])\n",
    "                writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# import csv\n",
    "# filepath = \"D:/MULTIMEDIA/MACHINE_LEARNING_THAY_QUANG/FUZZY SVM/CODE/07_04_2022/fuzzy_svm/Experiment/Data_Co_Author_200_1000_28092022_093028_Full.csv\"\n",
    "# time = datetime.now().strftime(\"%d%m%Y_%H%M%S\")\n",
    "# filename = \"Co_Author_200_1000\"\n",
    "# def compute_average_result(filepath,filename,time):\n",
    "#     data = pd.read_csv(filepath)\n",
    "#     sp_svm = se_svm = gm_svm = f1s_svm = acc_svm = auc_svm = 0\n",
    "#     sp_wsvm = se_wsvm = gm_wsvm = f1s_wsvm = acc_wsvm = auc_wsvm = 0\n",
    "#     sp_cen_lin = se_cen_lin = gm_cen_lin = f1s_cen_lin = acc_cen_lin = auc_cen_lin = 0\n",
    "#     sp_cen_exp = se_cen_exp = gm_cen_exp = f1s_cen_exp = acc_cen_exp = auc_cen_exp = 0\n",
    "#     sp_shp_lin = se_shp_lin = gm_shp_lin = f1s_shp_lin = acc_shp_lin = auc_shp_lin = 0\n",
    "#     sp_shp_exp = se_shp_exp = gm_shp_exp = f1s_shp_exp = acc_shp_exp = auc_shp_exp = 0\n",
    "#     sp_hyp_lin = se_hyp_lin = gm_hyp_lin = f1s_hyp_lin = acc_hyp_lin = auc_hyp_lin = 0\n",
    "#     sp_hyp_exp = se_hyp_exp = gm_hyp_exp = f1s_hyp_exp = acc_hyp_exp = auc_hyp_exp = 0\n",
    "#     sp_new1 = se_new1 = gm_new1 = f1s_new1 = acc_new1 = auc_new1 = 0\n",
    "#     sp_new2 = se_new2 = gm_new2 = f1s_new2 = acc_new2 = auc_new2 = 0\n",
    "#     sp_new3 = se_new3 = gm_new3 = f1s_new3 = acc_new3 = auc_new3 = 0\n",
    "\n",
    "#     for i in range(0,len(data)):\n",
    "#         # if(data['Times'][i] == '1'):\n",
    "#         if (data['Name Method'][i] == 'SVM') and (data['Name Function'][i] == 'SVM'):\n",
    "#             sp_svm = sp_svm + float(data['SP'][i])\n",
    "#             se_svm = se_svm + float(data['SE'][i])\n",
    "#             gm_svm = gm_svm + float(data['Gmean'][i])\n",
    "#             f1s_svm = f1s_svm + float(data['F1 Score'][i])\n",
    "#             acc_svm = acc_svm + float(data['Accuracy'][i])\n",
    "#             auc_svm = auc_svm + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'WSVM') and (data['Name Function'][i] == 'WSVM'):\n",
    "#             sp_wsvm = sp_wsvm + float(data['SP'][i])\n",
    "#             se_wsvm = se_wsvm + float(data['SE'][i])\n",
    "#             gm_wsvm = gm_wsvm + float(data['Gmean'][i])\n",
    "#             f1s_wsvm = f1s_wsvm + float(data['F1 Score'][i])\n",
    "#             acc_wsvm = acc_wsvm + float(data['Accuracy'][i])\n",
    "#             auc_wsvm = auc_wsvm + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'own_class_center') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "#             sp_cen_lin = sp_cen_lin + float(data['SP'][i])\n",
    "#             se_cen_lin = se_cen_lin + float(data['SE'][i])\n",
    "#             gm_cen_lin = gm_cen_lin + float(data['Gmean'][i])\n",
    "#             f1s_cen_lin = f1s_cen_lin + float(data['F1 Score'][i])\n",
    "#             acc_cen_lin = acc_cen_lin + float(data['Accuracy'][i])\n",
    "#             auc_cen_lin = auc_cen_lin + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'own_class_center') and (data['Name Function'][i] == 'exp'):\n",
    "#             sp_cen_exp = sp_cen_exp + float(data['SP'][i])\n",
    "#             se_cen_exp = se_cen_exp+ float(data['SE'][i])\n",
    "#             gm_cen_exp = gm_cen_exp + float(data['Gmean'][i])\n",
    "#             f1s_cen_exp = f1s_cen_exp + float(data['F1 Score'][i])\n",
    "#             acc_cen_exp = acc_cen_exp + float(data['Accuracy'][i])\n",
    "#             auc_cen_exp = auc_cen_exp + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'estimated_hyper_lin') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "#             sp_shp_lin = sp_shp_lin + float(data['SP'][i])\n",
    "#             se_shp_lin = se_shp_lin + float(data['SE'][i])\n",
    "#             gm_shp_lin = gm_shp_lin + float(data['Gmean'][i])\n",
    "#             f1s_shp_lin = f1s_shp_lin + float(data['F1 Score'][i])\n",
    "#             acc_shp_lin = acc_shp_lin + float(data['Accuracy'][i])\n",
    "#             auc_shp_lin = auc_shp_lin + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'estimated_hyper_lin') and (data['Name Function'][i] == 'exp'):\n",
    "#             sp_shp_exp = sp_shp_exp + float(data['SP'][i])\n",
    "#             se_shp_exp = se_shp_exp+ float(data['SE'][i])\n",
    "#             gm_shp_exp = gm_shp_exp + float(data['Gmean'][i])\n",
    "#             f1s_shp_exp = f1s_shp_exp + float(data['F1 Score'][i])\n",
    "#             acc_shp_exp = acc_shp_exp + float(data['Accuracy'][i])\n",
    "#             auc_shp_exp = auc_shp_exp + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'actual_hyper_lin') and (data['Name Function'][i] == 'lin_center_own'):\n",
    "#             sp_hyp_lin = sp_hyp_lin + float(data['SP'][i])\n",
    "#             se_hyp_lin = se_hyp_lin + float(data['SE'][i])\n",
    "#             gm_hyp_lin = gm_hyp_lin + float(data['Gmean'][i])\n",
    "#             f1s_hyp_lin = f1s_hyp_lin + float(data['F1 Score'][i])\n",
    "#             acc_hyp_lin = acc_hyp_lin + float(data['Accuracy'][i])\n",
    "#             auc_hyp_lin = auc_hyp_lin + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'actual_hyper_lin') and (data['Name Function'][i] == 'exp'):\n",
    "#             sp_hyp_exp = sp_hyp_exp + float(data['SP'][i])\n",
    "#             se_hyp_exp = se_hyp_exp + float(data['SE'][i])\n",
    "#             gm_hyp_exp = gm_hyp_exp + float(data['Gmean'][i])\n",
    "#             f1s_hyp_exp = f1s_hyp_exp + float(data['F1 Score'][i])\n",
    "#             acc_hyp_exp = acc_hyp_exp + float(data['Accuracy'][i])\n",
    "#             auc_hyp_exp = auc_hyp_exp + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new'):\n",
    "#             sp_new1 = sp_new1 + float(data['SP'][i])\n",
    "#             se_new1 = se_new1 + float(data['SE'][i])\n",
    "#             gm_new1 = gm_new1 + float(data['Gmean'][i])\n",
    "#             f1s_new1 = f1s_new1 + float(data['F1 Score'][i])\n",
    "#             acc_new1 = acc_new1 + float(data['Accuracy'][i])\n",
    "#             auc_new1 = auc_new1 + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new_v1'):\n",
    "#             sp_new2 = sp_new2 + float(data['SP'][i])\n",
    "#             se_new2 = se_new2 + float(data['SE'][i])\n",
    "#             gm_new2 = gm_new2 + float(data['Gmean'][i])\n",
    "#             f1s_new2 = f1s_new2 + float(data['F1 Score'][i])\n",
    "#             acc_new2 = acc_new2 + float(data['Accuracy'][i])\n",
    "#             auc_new2 = auc_new2 + float(data['AUC'][i])\n",
    "#         elif (data['Name Method'][i] == 'distance_center_own_opposite_tam') and (data['Name Function'][i] == 'func_own_opp_new_v2'):\n",
    "#             sp_new3= sp_new3 + float(data['SP'][i])\n",
    "#             se_new3 = se_new3 + float(data['SE'][i])\n",
    "#             gm_new3 = gm_new3 + float(data['Gmean'][i])\n",
    "#             f1s_new3 = f1s_new3 + float(data['F1 Score'][i])\n",
    "#             acc_new3 = acc_new3 + float(data['Accuracy'][i])\n",
    "#             auc_new3 = auc_new3 + float(data['AUC'][i])\n",
    "#         else:\n",
    "#             print(\"end\")\n",
    "#     a = 15\n",
    "#     header = ['Name Method', 'Name Function', 'SP', 'SE', 'Gmean', 'F1 Score','Accuracy','AUC']\n",
    "#     data = [['SVM','SVM',sp_svm/a,se_svm/a,gm_svm/a,f1s_svm/a,acc_svm/a,auc_svm/a],\n",
    "#             ['WSVM','WSVM',sp_wsvm/a,se_wsvm/a,gm_wsvm/a,f1s_wsvm/a,acc_wsvm/a,auc_wsvm/a],\n",
    "#             ['own_class_center','lin_center_own',sp_cen_lin/a,se_cen_lin/a,gm_cen_lin/a,f1s_cen_lin/a,acc_cen_lin/a,auc_cen_lin/a],\n",
    "#             ['own_class_center','exp',sp_cen_exp/a,se_cen_exp/a,gm_cen_exp/a,f1s_cen_exp/a,acc_cen_exp/a,auc_cen_exp/a],\n",
    "#             ['estimated_hyper_lin','lin_center_own',sp_shp_lin /a,se_shp_lin /a,gm_shp_lin /a,f1s_shp_lin /a,acc_shp_lin /a,auc_shp_lin /a],\n",
    "#             ['estimated_hyper_lin','exp',sp_shp_exp /a,se_shp_exp /a,gm_shp_exp/a,f1s_shp_exp/a,acc_shp_exp/a,auc_shp_exp/a],\n",
    "#             ['actual_hyper_lin','lin_center_own',sp_hyp_lin/a,se_hyp_lin/a,gm_hyp_lin/a,f1s_hyp_lin/a,acc_hyp_lin/a,auc_hyp_lin/a],\n",
    "#             ['actual_hyper_lin','exp',sp_hyp_exp/a,se_hyp_exp/a,gm_hyp_exp/a,f1s_hyp_exp/a,acc_hyp_exp/a,auc_hyp_exp/a],\n",
    "#             ['distance_center_own_opposite_tam','func_own_opp_new',sp_new1/a,se_new1/a,gm_new1/a,f1s_new1/a,acc_new1/a,auc_new1/a],\n",
    "#             ['distance_center_own_opposite_tam','func_own_opp_new_v1',sp_new2/a,se_new2/a,gm_new2/a,f1s_new2/a,acc_new2/a,auc_new2/a],\n",
    "#             ['distance_center_own_opposite_tam','func_own_opp_new_v2',sp_new3/a,se_new3/a,gm_new3/a,f1s_new3/a,acc_new3/a,auc_new3/a]]\n",
    "\n",
    "#     with open(f'./Experiment/Data_{filename}_{time}_Average.csv', 'a', encoding='UTF8', newline='') as f4:\n",
    "#         writer = csv.writer(f4)\n",
    "#         writer.writerow(header)\n",
    "#         writer.writerows(data)\n",
    "\n",
    "# compute_average_result(filepath, filename, time)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('FSVM-CIL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd073cf77417f2eda899b4964833636dfa63b2ff098a61069509a2b01c748e68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
